# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfMt-GGSiT26jj-usfXq3TyczzMthleN
"""

!pip install -q transformers datasets peft accelerate bitsandbytes sentence-transformers PyPDF2

from PyPDF2 import PdfReader #Python library for reading the crypto compliance pdf
from google.colab import files #google colab utility that - allows files to be uploaded from my local machine

uploaded = files.upload() #opens file upload box in colab - choose file and upload

pdf_path =list(uploaded.keys())[0]
reader = PdfReader(pdf_path) #create an object to read uploaded file


#loop through pages in PDF
#page.extract_text() - extract text from PDF
#Text object contains all readable content from the PDF
text = '\n'.join([page.extract_text() for page in reader.pages if page.extract_text()])

print(text[:1000])

#Here I am using a pretrained flan-t5-base model to generate Q&A pairs from the text we extracted from the PDF document
#This saves so much time in Creating Q&A pairs for fine tuning - I did not have to manually create them
#Shows that the model can answer questions about cryptocurrency

from transformers import pipeline

qa_gen = pipeline("text2text-generation", model="google/flan-t5-base") #load pretarined flan-tf-base model - text to text input - This model is used here to generate Q&A pairs from given chunks of text

# Chunk long text for question generation
chunks = [text[i:i+1000] for i in range(0, len(text), 1000)] #The text is split into chunks 1,000 charcaters each

qa_data = []
for chunk in chunks[:3]:  # process the first 3 chunks (this is only a demo in colab so we take the first 3 chunks to save on memory and speed)
    prompt = f"Generate 3 Q&A pairs from the following:\n\n{chunk}" #build instruction-style prompt for the model
    output = qa_gen(prompt, max_new_tokens=300)[0]['generated_text'] #generates up to 300 tokens in the response to the prompt --> extract the actual generated text via [0]['generated_text']
    print(output) #print the Q&A pairs
    qa_data.append(output) #append the qa_data list with the Q&A pairs

from transformers import AutoTokenizer

model_id = ('microsoft/phi-3-mini-4k-instruct')
tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)
# this function is designed to solit the long text into smaller chunks
#each chunk fits with the model toekn limit (2048 token for Phi-3, NOTE: THIS IS THE MAXIMUM TOKEN LIMIT for this model)
#Prevent overflow errors - keep logical flow between chunks


#text: the pdf text to split
#tokenizer: Hugging face tokenizer - text to tokens
#max_length as mentioned in the model is 2048
#stride: is like RecurrsiveCharacterTextSplitter - overlap between chunkc - context boundaries
def chunk_text_by_tokenizer(text, tokenizer, max_length=2048, stride=200):
    tokens = tokenizer.tokenize(text) #tokenize input text
    chunks = []
    start = 0
    while start < len(tokens): #begin loop to go through tokens in max_length
        end = min(start + max_length, len(tokens))
        chunk_tokens = tokens[start:end]
        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens) #convert tokens back to a readable string
        chunks.append(chunk_text) #append the empty list created above
        start += max_length - stride  # creatie overlap between chunks
    return chunks #Returns a list of clean, token-aware, overlapping text chunks ready for generation, Q&A, or fine-tuning

# Example usage:
chunks = chunk_text_by_tokenizer(text, tokenizer, max_length=2048, stride=200) #calling the function on the text
print(f"Created {len(chunks)} chunks") #how many chunks have been created from this PDF file

from datasets import Dataset #Hugging face dataset library - used to create the dataset from the text (chunks)

data_dict = {"text": chunks} #create dict where 'text' is the column name and the value is the chunks we created
dataset = Dataset.from_dict(data_dict) # convert the dict to a Hugging Face dataset object - integrate this dataset with the transformers for fine-tuning

from transformers import AutoModelForCausalLM #text generation model
from peft import LoraConfig, get_peft_model, TaskType #peft model import LoRA to fin-tune model efficently - train a small number of selected parameters

model_id = "microsoft/phi-3-mini-4k-instruct" #I added this code again just to show the model I am fine tuning

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto", # works with aviailable memory either CPU or GPU auromatically
    load_in_4bit=True, #quantizing the model to save on GPU memory and faster infernce -- bitsandbytes needs to be installed for this to be implmented and torch
    trust_remote_code=True #allows loading custom model implementations
)

tokenizer.pad_token = tokenizer.eos_token  # Phi-3 doesnt define a pad token - Use the eos_token (end-of-sequence token) wherever padding is needed.

#Inspect model layers in code
for name, module in model.named_modules():
    if "qkv_proj" in name:
        print(name)

lora_config = LoraConfig(
    r=8, #rank how many parameters are added
    lora_alpha=16, #scaling factor that controls how much the LoRA weights influence the model during training
    target_modules=['qkv_proj'], #specifies the layers of the base model created that will be adapted by LoRA - "qkv_proj" module handles the Query, Key, and Value projection in attention layers.
    lora_dropout=0.1, #regularization during training and prevent overfitting
    bias="none",
    task_type=TaskType.CAUSAL_LM, # indicates causal language modelling - indicates to LoRA how to apply properly
)

model = get_peft_model(model, lora_config) #wrapper base model with the LoRA adapters - base model stays frozen not trainable - only 'qkv_proj' layers will be trained

def tokenize_function(example):
    tokens = tokenizer(example["text"], truncation=True, padding="max_length", max_length=512) #tokenizer the text
    tokens["labels"] = tokens["input_ids"].copy()  # IMPRTANT: training labels for Causal language Modleling - training labels should be the same as the input IDS - predict each token from previous tokens
    return tokens

tokenized_dataset = dataset.map(tokenize_function) #map the dataset to the tokenize_function - return a new dataset with all the data toeknized and labels

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./phi3-crypto-lora", #model checkpoints,logs and outs are saved during training
    per_device_train_batch_size=1, #no. of training examples processed by each GU/CPU at a time - set to 1 to reduce memory usage
    num_train_epochs=3,#sets how many times the entire dataset is bassed throught he model
    logging_steps=10, #log training metrics every 10 steps -- monitoring model progress
    save_steps=10,
    save_total_limit=1,
    report_to=[], #disable all logging integrations (wandb)
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset, #pass the tokenized_dataset - we created when mapping the dataset to the tokenize_function
)

trainer.train() #start the training process

model.eval() #model evalauation mode (disable dropout)

prompt = "<|user|>\nSummarize the key crypto risks in 2025.\n<|assistant|>\n" #<|user|> --> special token that marks the beginning of the userâ€™s input or question --> <|assistant|>: special token signals the start of the models response --(not respond to the question)
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)#tokenize the prompt - returning PyTorch Tensors

outputs = model.generate(**inputs, max_new_tokens=200,use_cache=False)#disables caching of key/value pairs during generation - limit geerated response to 200 tokens
print(tokenizer.decode(outputs[0], skip_special_tokens=True))#decode the output

#for name, module in model.named_modules():
    #print(name)

#for name, module in model.named_modules():
    #if "qkv_proj" in name:
        #print(name)