# Fine-Tuning Phi-3 Mini with LoRA on a Crypto Compliance Report

##Project Overview
- This project demonstrates how to fine-tune the microsoft/phi-3-mini-4k-instruct Large Language Model (LLM) using LoRA (Low-Rank Adaptation) for domain adaptation. We inject new knowledge into the model using a PDF report on cryptocurrency compliance in 2025. The model is then able to answer questions based on the newly learned context â€” without full model retraining.

- Designed to work entirely in Google Colab using 4-bit quantization, this project is resource-efficient and perfect for extending open-source LLMs with domain-specific updates.
